---
title: "Shellfish Sanitation Program Data Assembly"
output: html_notebook
---
# INtroduction
Initial loading of the raw observational data uncovered what appear to be data format inconsistnecies with the way the E. coli data were recorded.Here we explore th e data inconsiustencies and make decisions regarding how to address them.



# Install Libraries
```{r}
library(readr)
library(tidyverse)
```

# Load Data
Most of this code is to skip data columns that were used for data QA/QC, or to ensure dates and times are properly interpreted.
```{r}
sibfldnm <- 'Original Data'
parent <- dirname(getwd())
sibling <- file.path(parent,sibfldnm)
fn<- "Casco Bay WQ 15.19.csv"
path <- file.path(sibling, fn)
raw_data <- read_csv(path, 
    col_types = cols(
        ADVERSITY = col_skip(),  
        CATCH_COMMENTS = col_skip(), 
        CATCH_SEQ_NO = col_skip(),
        CATCH_UPDATE_DATE = col_skip(), 
        CATCH_UPDATE_USER = col_skip(),
        COL_METHOD = col_skip(), 
        DMR_CATCH_IDENTIFIER = col_skip(), 
        DMR_EFFORT_IDENTIFIER = col_skip(), 
        DMR_SAMPLE_IDENTIFIER = col_skip(), 
        EFFORT_COMMENTS = col_skip(),
        EFFORT_SEQ_NO = col_skip(), 
        EFFORT_START_DATE = col_date(format = "%Y-%m-%d"), 
        EFFORT_START_TIME = col_time(format = "%H:%M"), 
        EFFORT_UPDATE_DATE = col_skip(), 
        EFFORT_UPDATE_USER = col_skip(), 
        EXAM_DATE_TIME = col_skip(),
        INITIATED_BY = col_skip(), 
        INITIATED_DATE = col_skip(),
        LAB = col_skip(), 
        LAT_DD = col_skip(),
        LON_DD = col_skip(), 
        MISSED_STATION_CODE = col_skip(),
        ROW_NUMBER = col_skip(),
        SAMPLE_COMMENTS = col_skip(),
        SAMPLE_METHOD = col_skip(),
        SAMPLE_SEQ_NO = col_skip(), 
        SAMPLE_UPDATE_DATE = col_skip(), 
        SAMPLE_UPDATE_USER = col_skip(),
        STRATEGY = col_skip(),  
        TRIP_SEQ_NO = col_skip(),
        X = col_skip(),
        X1 = col_skip())) %>%
  mutate_at(c('LOCATION_ID', 'GROWING_AREA', 'OPEN_CLOSED_FLAG',
              'WIND_DIRECTION','TIDE_STAGE',
              'CURRENT_CLASSIFICATION_CODE',
              'CATEGORY'), factor) %>%
  mutate(YEAR = as.numeric(format(EFFORT_START_DATE, "%Y"))) %>%
  mutate(DOY = as.numeric(format(EFFORT_START_DATE, '%j'))) %>%
  mutate(MONTH = as.numeric(format(EFFORT_START_DATE, '%m')))
```

```{r}
with(raw_data, xtabs(~factor(format(EFFORT_START_DATE, '%m')) +factor(YEAR)))
```
Note that we the data sent to us does includes fewer November and no December observations in 2019.  We may be better off restricting our analysis to 2015-2018.  Althoug hwe can check for seasonal patterns in the data as a check on the necessity for that.
## Convert Orders of Factors
Since these are factors, and not ordered factors, this is merely to make it a bit easier to interpret results.It also has the effect of changing the base category for contrasts.  Note that this will not affect ordering of factors when data is loaded in from CSV files, so this code (or something very like it) will have to be run in any data analysis scripts.
## What levels are present?
```{r}
with(raw_data, levels(OPEN_CLOSED_FLAG))
with(raw_data, levels(WIND_DIRECTION))
with(raw_data, levels(TIDE_STAGE))
with(raw_data, levels(CURRENT_CLASSIFICATION_CODE))
with(raw_data, levels(CATEGORY))
```
## Reorder Some
```{r}
raw_data <- raw_data %>%
  mutate(WIND_DIRECTION =factor(WIND_DIRECTION,
                                levels = c("CL", "N", "NNE", "NE",
                                           "E","SE", "S","SW",
                                           "W", "NW"))) %>%
  mutate(CURRENT_CLASSIFICATION_CODE = factor(CURRENT_CLASSIFICATION_CODE,
                                              levels = c( 'A', 'CA', 'CR',
                                                          'R', 'P', 'X' ))) %>%
  mutate(TIDE_STAGE = factor(TIDE_STAGE, levels = c("L", "LF", "F", "HF",
                                                    "H", "HE", "E", "LE")))
```
## Simplify Column Names
Data names from DMR are long, in all caps, and awkward, so we want to simplify.  While we are at it, we want names to more or less match what we found in the p90 files.
```{r}
names(raw_data)
```

```{r}
raw_data <- raw_data %>%
  select(-FLOOD, -DELIVERY_TEMP_C) %>%
  rename(SDate = EFFORT_START_DATE,
         STime = EFFORT_START_TIME,
         SDateTime = START_DATE_TIME,
         Station =  LOCATION_ID,
         GROW_AREA =  GROWING_AREA,
         OpenClosed = OPEN_CLOSED_FLAG,
         WDIR = WIND_DIRECTION,
         Tide =  TIDE_STAGE,
         Class = CURRENT_CLASSIFICATION_CODE,
         Temp = TEMP_C,
         Sal = SALINITY_PCT,
         ColiScore = COL_SCORE,    # This value uses inapporpriate substitution based on right censored values.
         RawColi = RAW_COL_SCORE)
```


```{r}
with(raw_data, sum(OpenClosed=='X', na.rm=TRUE))
with(raw_data, xtabs(~OpenClosed+factor(YEAR)))
```
So, the "X" flag was not used prior to 2018, and used infrequently in 2019.  We need to understand what that flag means, but it may be appropriate for some analyses to delete records with the X flag....

```{r}
with(raw_data, levels(WDIR))
with(raw_data, xtabs(~WDIR+factor(YEAR)))
```
Only "NW" was recorded in 2019. Most 2019 observations have no wind direction code. NNE was only used in 2016. Otherwoise, these data look inconsistent year to year, so they may not be all that useful for analysis.

```{r}
with(raw_data, xtabs(~OpenClosed+CATEGORY))
```
So CATEGORY contains no useful information for us.
```{r}
with(raw_data, xtabs(~OpenClosed+Class))
```
Note that there's a (nearly) one to one match between category = Z, OPenClosed=X and  Class= X.  Also note that Category = I was only used once.

I'd like to know what the meaning of the OpenCLosed flag really is.  Was this a flag showing condition before the sample, or a classification due to the results of the sample.  We should be able to determine that based on the data itself.

## Remove Unreliable / Uninformative Categories
This code removes uninformative or inconsistent data and replaces "X" values with NA.
```{r}
raw_data<-raw_data %>%
  select(-CATEGORY, -WDIR) %>%
  mutate(Class = factor(Class,levels = c( 'A', 'CA', 'CR',
                                          'R', 'P'))) %>%
  mutate(OpenClosed = factor(OpenClosed, levels = c('O', 'C')))
```

# Consistency of How Censored Data is Reported
```{r}
the_data <- raw_data %>%
  # Remove records with NAs for hte RawColi scores
  filter (! is.na(RawColi)) %>%
  #Identify censored data
  mutate(LCFlag = substr(RawColi,1,1)=='<') %>%
  mutate(RCFlag = substr(RawColi,1,1)=='>') %>%
  mutate(ColiVal = if_else(LCFlag,
                           substr(RawColi,2,nchar(RawColi)),
                           RawColi )) %>%
  mutate(ColiVal = if_else(RCFlag,
                           substr(RawColi,2,nchar(RawColi)),
                           ColiVal)) %>%
  mutate(ColiVal = as.numeric(ColiVal))
 
```

```{r}
ggplot(the_data, aes(ColiVal, ColiScore)) +
  geom_point() +
  geom_abline(slope = 1, intercept = 0)
```
So, here we see a couple of things:
1. DMR replaced "raw" observations with arbitary larrger or smaller values when they produced the "ColiScore values.  We may be able to do better, or handle these censored data with more sophistication.
2.  there's a strange observation or group of observations that was recorded as < something that is wel labove the nominal detection limits.

## Left Censoring
```{r}
ggplot(the_data[the_data$ColiScore<5,], aes(ColiVal, ColiScore)) +
  geom_point(alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0)
```
so we have some inconsistency of how censored observations at the low end were handled.  The detection limit with the methods is presumably consistent at 2.0  but apparently 1.9 was recorded in some places.
```{r}
sum(the_data$ColiVal==1.9 & the_data$ColiScore == 1.9, na.rm=TRUE)
sum(the_data$ColiVal==1.9, na.rm=TRUE)
```
So, those inconsistencies are very rare, and can be corrected with little concern for loss of accuracy.
```{r}
the_data$ColiVal[the_data$ColiVal==1.9 ] = 2.0
```
What about other inconsistencies, with regard to what values are flagged as left censored?
```{r}
the_data %>%
  filter(LCFlag) %>%
  group_by(ColiVal) %>%
  summarise(L = first(ColiVal)) %>%
  pull(L) 
```

```{r}
the_data[the_data$ColiVal==18 & the_data$LCFlag, ]
```
For these three sample, DMR interpreted them consistently, assuming the real meaning of these samples is that they were left censored below a detection limit of 18.  The observations were all from a single day.  This elevated detection limit is problematic, as it will bias any unsophisticated handling of non-detects.  We could consider removing these observations as anomolous, but there is no internal justification for latering these values.

# Right Censoring
```{r}
ggplot(the_data[the_data$ColiScore>1000,], aes(ColiVal, ColiScore)) +
  geom_point(alpha = 0.1) +
  geom_abline(slope = 1, intercept = 0)
```
```{r}
the_data %>%
filter(ColiVal>1500) 
```
So, DMR used an arbitrary value of 1700 as being "slightly" higher than the 1600 upper detection limit.

# Data export
```{r}
cn <- colnames(the_data)
write.table(the_data, 'Shellfish data 2015 2018.csv', sep = ',',
            na='', row.names = FALSE, col.names = cn)
```

# Final Data Cleanup
We may want to directly load the data produces in this Notebook for use in analysis, rather than importing it from the CSV file, so we might as well clean things up.

```{r}
rm(raw_data, cn, fn,  path, parent, sibfldnm, sibling)
```

